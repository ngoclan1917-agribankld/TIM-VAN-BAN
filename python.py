import streamlit as st
from docx import Document
import mammoth
from unidecode import unidecode
import io
import re
import hashlib

# ==========================
# ‚öôÔ∏è C·∫§U H√åNH GIAO DI·ªÜN
# ==========================
st.set_page_config(
    page_title="üìÑ Tra c·ª©u vƒÉn b·∫£n Word",
    page_icon="üìò",
    layout="wide"
)

st.title("üìÑ ·ª®NG D·ª§NG TRA C·ª®U N·ªòI DUNG VƒÇN B·∫¢N (.DOC, .DOCX)")
st.markdown(
    """
    - üìÇ **B√™n tr√°i:** T·∫£i file `.doc`, `.docx` c·∫ßn tra c·ª©u  
    - üîé **B√™n ph·∫£i:** Nh·∫≠p t·ª´ kh√≥a ‚Üí Nh·∫•n **Enter** ho·∫∑c n√∫t **"T√¨m ki·∫øm"** ƒë·ªÉ xem c√°c ƒëo·∫°n ch·ª©a t·ª´ kh√≥a k√®m ng·ªØ c·∫£nh 3‚Äì4 c√¢u.
    """
)

# ==========================
# ‚öôÔ∏è H·∫∞NG S·ªê
# ==========================
CONTEXT_BEFORE = 3   # s·ªë c√¢u tr∆∞·ªõc t·ª´ kh√≥a
CONTEXT_AFTER = 3    # s·ªë c√¢u sau t·ª´ kh√≥a

# ==========================
# üß© H√ÄM X·ª¨ L√ù C∆† B·∫¢N
# ==========================

def extract_text_from_docx(file_bytes: bytes) -> str:
    """ƒê·ªçc n·ªôi dung t·ª´ file .docx, tr·∫£ v·ªÅ text ƒë∆°n gi·∫£n, gi·ªØ xu·ªëng d√≤ng."""
    doc = Document(io.BytesIO(file_bytes))
    paragraphs = []
    for p in doc.paragraphs:
        text = p.text.strip()
        if text:
            paragraphs.append(text)
    return "\n".join(paragraphs)


def extract_text_from_doc(file_bytes: bytes) -> str:
    """ƒê·ªçc n·ªôi dung t·ª´ file .doc b·∫±ng mammoth, chuy·ªÉn HTML -> text ƒë∆°n gi·∫£n."""
    result = mammoth.convert_to_html(io.BytesIO(file_bytes))
    html = result.value
    # B·ªè tag HTML ƒë∆°n gi·∫£n ƒë·ªÉ l·∫•y text
    text = re.sub(r"<[^>]+>", " ", html)
    text = re.sub(r"\s+\n", "\n", text)
    text = re.sub(r"\n\s+", "\n", text)
    text = re.sub(r"\s{2,}", " ", text)
    return text.strip()


def split_into_sentences(text: str):
    """
    T√°ch c√¢u t·ªëi ∆∞u cho vƒÉn b·∫£n quy ƒë·ªãnh / ti·∫øng Vi·ªát.
    Kh√¥ng ho√†n h·∫£o 100%, nh∆∞ng ƒë·ªß nhanh & ·ªïn ƒë·ªãnh.
    """
    # Chu·∫©n h√≥a xu·ªëng d√≤ng th√†nh d·∫•u ph√¢n t√°ch nh·∫π
    normalized = text.replace("\r", "\n")
    normalized = re.sub(r"\n+", "\n", normalized)

    # T·∫°m th·ªùi thay xu·ªëng d√≤ng b·∫±ng k√Ω hi·ªáu ƒë·∫∑c bi·ªát ƒë·ªÉ gi·ªØ c·∫•u tr√∫c ƒëo·∫°n
    placeholder = " <NL> "
    normalized = normalized.replace("\n", placeholder)

    # Regex t√°ch c√¢u: sau . ! ? ‚Ä¶ ; r·ªìi c√≥ kho·∫£ng tr·∫Øng + ch·ªØ c√°i/ s·ªë / m·ªü ngo·∫∑c / ngo·∫∑c k√©p
    pattern = r'(?<=[\.!\?‚Ä¶;])\s+(?=[A-Z√Ä-·ª¥√Ç√ä√î∆†∆Øƒê0-9‚Äú"(\[])'
    raw_sentences = re.split(pattern, normalized)

    sentences = []
    for s in raw_sentences:
        s = s.strip()
        if not s:
            continue
        # Tr·∫£ l·∫°i xu·ªëng d√≤ng
        s = s.replace(placeholder, "\n")
        # Lo·∫°i b·ªè c√¢u qu√° ng·∫Øn r√°c
        if len(s) > 1:
            sentences.append(s)
    return sentences


def normalize_for_search(text: str) -> str:
    """Chu·∫©n h√≥a ƒë·ªÉ t√¨m ki·∫øm: b·ªè d·∫•u, lower."""
    return unidecode(text).lower()


def highlight_keyword(text: str, keywords):
    """
    T√¥ ƒë·∫≠m/b√¥i v√†ng t·ª´ kh√≥a trong ƒëo·∫°n k·∫øt qu·∫£.
    keywords: list t·ª´ kh√≥a g·ªëc (gi·ªØ nguy√™n d·∫•u).
    """
    if not keywords:
        return text

    # S·∫Øp x·∫øp t·ª´ kh√≥a d√†i tr∆∞·ªõc ƒë·ªÉ tr√°nh l·ªìng nhau
    keywords_sorted = sorted(set([k for k in keywords if k.strip()]), key=len, reverse=True)

    def repl_factory(pattern):
        regex = re.compile(pattern, flags=re.IGNORECASE)

        def _repl(match):
            return f"<mark><b>{match.group(0)}</b></mark>"
        return regex, _repl

    result = text
    for kw in keywords_sorted:
        pattern = re.escape(kw)
        regex, repl = repl_factory(pattern)
        result = regex.sub(repl, result)

    return result


# ==========================
# üß† CACHE X·ª¨ L√ù FILE
# ==========================

@st.cache_data(show_spinner=False)
def build_index(files_payload):
    """
    T·ª´ danh s√°ch (filename, bytes) ‚Üí tr·∫£ v·ªÅ c·∫•u tr√∫c:
    [
      {
        "file_name": str,
        "sentences": [str, ...],
        "norm_sentences": [str, ...]  # ƒë·ªÉ t√¨m ki·∫øm nhanh
      },
      ...
    ]
    """
    indexed_docs = []

    for file_name, file_bytes in files_payload:
        ext = file_name.lower().split(".")[-1]

        try:
            if ext == "docx":
                text = extract_text_from_docx(file_bytes)
            elif ext == "doc":
                text = extract_text_from_doc(file_bytes)
            else:
                continue

            if not text:
                continue

            sentences = split_into_sentences(text)
            norm_sentences = [normalize_for_search(s) for s in sentences]

            if sentences:
                indexed_docs.append(
                    {
                        "file_name": file_name,
                        "sentences": sentences,
                        "norm_sentences": norm_sentences,
                    }
                )
        except Exception as e:
            # Ghi log ra UI n·∫øu c·∫ßn debug
            st.warning(f"Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c file: {file_name}. L·ªói: {e}")

    return indexed_docs


def search_keyword(indexed_docs, query_raw: str, before=3, after=3, max_results_per_file=200):
    """
    T√¨m ki·∫øm theo t·ª´ kh√≥a, tr·∫£ v·ªÅ danh s√°ch k·∫øt qu·∫£:
    [
      {
        "file_name": ...,
        "context": "ƒëo·∫°n tr√≠ch 3-4 c√¢u tr∆∞·ªõc/sau c√≥ highlight"
      },
      ...
    ]
    H·ªó tr·ª£ nh·∫≠p nhi·ªÅu t·ª´ kh√≥a, ngƒÉn c√°ch b·∫±ng d·∫•u ; ho·∫∑c ,
    ƒêi·ªÅu ki·ªán: c√¢u ch·ª©a B·∫§T K·ª≤ t·ª´ kh√≥a n√†o (OR).
    """
    if not query_raw:
        return []

    # T√°ch nhi·ªÅu t·ª´ kh√≥a n·∫øu c√≥
    raw_parts = [p.strip() for p in re.split(r"[;,]", query_raw) if p.strip()]
    if not raw_parts:
        return []

    norm_keywords = [normalize_for_search(p) for p in raw_parts]

    results = []

    for doc in indexed_docs:
        file_name = doc["file_name"]
        sentences = doc["sentences"]
        norm_sentences = doc["norm_sentences"]

        hits = []

        for i, s_norm in enumerate(norm_sentences):
            if any(kw in s_norm for kw in norm_keywords):
                hits.append(i)

        if not hits:
            continue

        # Gom v√† t·∫°o context
        used_ranges = []
        file_results = []

        for hit_idx in hits:
            start = max(0, hit_idx - before)
            end = min(len(sentences), hit_idx + after + 1)

            # Tr√°nh tr√πng l·∫∑p v√πng v·ªõi k·∫øt qu·∫£ tr∆∞·ªõc
            if used_ranges and start <= used_ranges[-1][1]:
                # merge
                used_ranges[-1] = (used_ranges[-1][0], max(used_ranges[-1][1], end))
            else:
                used_ranges.append((start, end))

        for (start, end) in used_ranges:
            snippet_sentences = sentences[start:end]
            snippet_text = " ".join(snippet_sentences).strip()
            snippet_text = re.sub(r"\s{2,}", " ", snippet_text)
            snippet_html = highlight_keyword(snippet_text, raw_parts)
            file_results.append(snippet_html)
            if len(file_results) >= max_results_per_file:
                break

        for snippet_html in file_results:
            results.append(
                {
                    "file_name": file_name,
                    "context_html": snippet_html
                }
            )

    return results


# ==========================
# üñ•Ô∏è GIAO DI·ªÜN 2 C·ªòT
# ==========================

col1, col2 = st.columns([1, 2])

with col1:
    st.subheader("üìÇ T·∫£i vƒÉn b·∫£n")
    uploaded_files = st.file_uploader(
        "Ch·ªçn m·ªôt ho·∫∑c nhi·ªÅu file .doc / .docx",
        type=["doc", "docx"],
        accept_multiple_files=True
    )

    if uploaded_files:
        st.success(f"ƒê√£ t·∫£i {len(uploaded_files)} file.")
        for f in uploaded_files:
            st.markdown(f"- {f.name} ({f.size/1024:.1f} KB)")
    else:
        st.info("Vui l√≤ng t·∫£i l√™n √≠t nh·∫•t m·ªôt file ƒë·ªÉ b·∫Øt ƒë·∫ßu tra c·ª©u.")

with col2:
    st.subheader("üîç Tra c·ª©u t·ª´ kh√≥a")

    # Form ƒë·ªÉ h·ªó tr·ª£ Enter = Submit
    with st.form("search_form", clear_on_submit=False):
        default_query = st.session_state.get("last_query", "")
        query = st.text_input(
            "Nh·∫≠p t·ª´ kh√≥a (c√≥ th·ªÉ nh·∫≠p nhi·ªÅu, c√°ch nhau b·ªüi d·∫•u ';' ho·∫∑c ',')",
            value=default_query,
            placeholder="V√≠ d·ª•: h·∫°n m·ª©c t√≠n d·ª•ng; t√†i s·∫£n b·∫£o ƒë·∫£m; ƒëi·ªÅu ki·ªán vay"
        )
        submitted = st.form_submit_button("üîç T√¨m ki·∫øm")

    if submitted:
        st.session_state["last_query"] = query

        if not uploaded_files:
            st.warning("Vui l√≤ng t·∫£i file ·ªü b√™n tr√°i tr∆∞·ªõc khi t√¨m ki·∫øm.")
        elif not query.strip():
            st.warning("Vui l√≤ng nh·∫≠p t·ª´ kh√≥a c·∫ßn tra c·ª©u.")
        else:
            # Chu·∫©n b·ªã d·ªØ li·ªáu cho cache: (t√™n, bytes)
            files_payload = []
            for uf in uploaded_files:
                content = uf.getvalue()
                # ƒë·ªÉ cache hi·ªáu qu·∫£ h∆°n: th√™m hash
                file_hash = hashlib.md5(content).hexdigest()
                files_payload.append((f"{uf.name}::{file_hash}", content))

            with st.spinner("ƒêang x·ª≠ l√Ω & tra c·ª©u..."):
                indexed_docs = build_index(files_payload)
                results = search_keyword(
                    indexed_docs,
                    query_raw=query,
                    before=CONTEXT_BEFORE,
                    after=CONTEXT_AFTER
                )

            st.markdown("---")
            if not results:
                st.warning("Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£ n√†o ch·ª©a t·ª´ kh√≥a trong c√°c file ƒë√£ t·∫£i.")
            else:
                st.success(f"T√¨m th·∫•y {len(results)} ƒëo·∫°n ph√π h·ª£p trong c√°c vƒÉn b·∫£n.")
                for i, item in enumerate(results, start=1):
                    st.markdown(
                        f"""
                        <div style="padding:10px; margin-bottom:8px; border-radius:6px; border:1px solid #ddd;">
                            <div style="font-size:13px; color:#555;">
                                <b>File:</b> {item['file_name'].split("::")[0]} &nbsp;|&nbsp; <b>K·∫øt qu·∫£ #{i}</b>
                            </div>
                            <div style="margin-top:4px; font-size:14px; line-height:1.6;">
                                {item['context_html']}
                            </div>
                        </div>
                        """,
                        unsafe_allow_html=True
                    )
