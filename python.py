import streamlit as st
import pandas as pd
from io import BytesIO
from docx import Document
import tempfile
import os
import subprocess
import re
import nltk

# Táº£i bá»™ tÃ¡ch cÃ¢u (náº¿u chÆ°a cÃ³)
try:
    nltk.data.find("tokenizers/punkt")
except LookupError:
    nltk.download("punkt")

# ==========================
# âš™ï¸ Cáº¤U HÃŒNH GIAO DIá»†N
# ==========================
st.set_page_config(page_title="ğŸ“˜ Tra cá»©u VÄƒn báº£n Word", page_icon="ğŸ“„", layout="wide")
st.title("ğŸ“˜ á»¨NG Dá»¤NG TRA Cá»¨U Ná»˜I DUNG VÄ‚N Báº¢N WORD")
st.markdown("ğŸ“‚ **BÃªn trÃ¡i:** Táº£i file DOC/DOCX â€” ğŸ’¬ **BÃªn pháº£i:** Nháº­p tá»« khÃ³a Ä‘á»ƒ tÃ¬m kiáº¿m ná»™i dung.")

# ==========================
# ğŸ§  SESSION STATE
# ==========================
if "uploaded_files" not in st.session_state:
    st.session_state.uploaded_files = {}
if "uploader_key" not in st.session_state:
    st.session_state.uploader_key = 0

# ==========================
# ğŸ¨ CSS TÃ™Y CHá»ˆNH
# ==========================
st.markdown("""
<style>
div[data-testid="column"]:first-child { margin-right: 60px !important; }
.highlight-red { color: red; font-weight: bold; }
.text-block { white-space: pre-wrap; font-family: 'Times New Roman', serif; line-height: 1.6; }
</style>
""", unsafe_allow_html=True)

# ==========================
# ğŸ“‚ HÃ€M Äá»ŒC FILE DOC/DOCX
# ==========================
def read_text_from_file(file):
    """Äá»c ná»™i dung tá»« file DOC hoáº·c DOCX"""
    text = ""
    ext = file.name.lower().split(".")[-1]

    try:
        if ext == "docx":
            doc = Document(file)
            text = "\n".join(p.text for p in doc.paragraphs)

        elif ext == "doc":
            with tempfile.NamedTemporaryFile(delete=False, suffix=".doc") as tmp_doc:
                tmp_doc.write(file.read())
                tmp_doc_path = tmp_doc.name
            tmp_docx_path = tmp_doc_path + "x"
            try:
                subprocess.run(
                    ["soffice", "--headless", "--convert-to", "docx",
                     "--outdir", os.path.dirname(tmp_docx_path), tmp_doc_path],
                    check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE
                )
                doc = Document(tmp_docx_path)
                text = "\n".join(p.text for p in doc.paragraphs)
            except Exception as e:
                st.error(f"âŒ KhÃ´ng thá»ƒ Ä‘á»c file DOC ({file.name}): {e}")
            finally:
                for path in [tmp_doc_path, tmp_docx_path]:
                    if os.path.exists(path):
                        os.remove(path)
        else:
            st.warning("âš ï¸ Chá»‰ há»— trá»£ file DOC hoáº·c DOCX.")
    except Exception as e:
        st.error(f"âŒ Lá»—i Ä‘á»c file {file.name}: {e}")

    return text.strip()

# ==========================
# ğŸ” HÃ€M TÃŒM KIáº¾M NGáº®T CÃ‚U Äá»¦ Ã
# ==========================
def tim_trong_van_ban(keyword, dataframe):
    """TÃ¬m Ä‘oáº¡n vÄƒn cÃ³ chá»©a tá»« khÃ³a, ngáº¯t cÃ¢u Ä‘á»§ Ã½"""
    kw = keyword.strip().lower()
    results = []

    for _, row in dataframe.iterrows():
        sentences = nltk.sent_tokenize(row["Ná»˜I_DUNG"])
        matched_blocks = []

        for i, sentence in enumerate(sentences):
            if kw in sentence.lower():
                # Má»Ÿ rá»™ng 1â€“3 cÃ¢u tÃ¹y ngá»¯ cáº£nh Ä‘á»ƒ Ä‘áº£m báº£o Ä‘á»§ Ã½
                start = max(0, i - 2)
                end = min(len(sentences), i + 3)
                snippet = " ".join(sentences[start:end]).strip()
                matched_blocks.append(snippet)

        for block in matched_blocks:
            results.append({
                "TRICH_DOAN": block,
                "TÃŠN_FILE": row["TÃŠN_FILE"]
            })
    return pd.DataFrame(results)

# ==========================
# ğŸ§­ 2 Cá»˜T GIAO DIá»†N
# ==========================
col1, col2 = st.columns([1, 2])

# ==========================
# ğŸ“ Cá»˜T TRÃI â€” Táº¢I FILE
# ==========================
with col1:
    st.subheader("ğŸ“‚ Táº£i file Word")

    uploaded_files = st.file_uploader(
        "Chá»n file (.doc hoáº·c .docx, cÃ³ thá»ƒ nhiá»u)",
        type=["doc", "docx"],
        accept_multiple_files=True,
        key=f"uploader_{st.session_state.uploader_key}"
    )

    if uploaded_files:
        for file in uploaded_files:
            if file.name not in st.session_state.uploaded_files:
                text_content = read_text_from_file(file)
                if text_content:
                    df = pd.DataFrame({"Ná»˜I_DUNG": [text_content], "TÃŠN_FILE": [file.name]})
                    st.session_state.uploaded_files[file.name] = df
                    st.success(f"âœ… ÄÃ£ táº£i: {file.name}")
                else:
                    st.warning(f"âš ï¸ KhÃ´ng thá»ƒ trÃ­ch xuáº¥t ná»™i dung tá»«: {file.name}")

    if st.session_state.uploaded_files:
        if st.button("ğŸ§¹ XÃ³a táº¥t cáº£ file"):
            st.session_state.uploaded_files.clear()
            st.session_state.uploader_key += 1
            st.rerun()

# ==========================
# ğŸ’¬ Cá»˜T PHáº¢I â€” TRA Cá»¨U
# ==========================
with col2:
    st.subheader("ğŸ’¬ Tra cá»©u ná»™i dung")

    if st.session_state.uploaded_files:
        combined_df = pd.concat(st.session_state.uploaded_files.values(), ignore_index=True)

        user_input = st.text_input("ğŸ” Nháº­p tá»« khÃ³a cáº§n tÃ¬m (Enter hoáº·c nháº¥n nÃºt):", key="search_input")
        search_btn = st.button("ğŸ” TÃ¬m kiáº¿m")

        if (user_input and st.session_state.search_input) or search_btn:
            keyword = user_input.strip()
            if keyword:
                results = tim_trong_van_ban(keyword, combined_df)
                if results.empty:
                    st.warning("âŒ KhÃ´ng tÃ¬m tháº¥y ná»™i dung nÃ o phÃ¹ há»£p.")
                else:
                    for _, row in results.iterrows():
                        highlighted = re.sub(
                            fr"({re.escape(keyword)})",
                            r'<span class="highlight-red">\1</span>',
                            row["TRICH_DOAN"],
                            flags=re.IGNORECASE
                        )
                        st.markdown(f'<div class="text-block">{highlighted}</div>', unsafe_allow_html=True)
                        st.caption(f"ğŸ“ Nguá»“n: *{row['TÃŠN_FILE']}*")
                        st.divider()
            else:
                st.info("âš ï¸ Nháº­p tá»« khÃ³a Ä‘á»ƒ tÃ¬m kiáº¿m.")
    else:
        st.info("ğŸ“Œ HÃ£y táº£i Ã­t nháº¥t má»™t file DOC/DOCX Ä‘á»ƒ báº¯t Ä‘áº§u.")

# ==========================
# ğŸ“˜ HÆ¯á»šNG DáºªN
# ==========================
with st.expander("ğŸ“˜ HÆ°á»›ng dáº«n sá»­ dá»¥ng"):
    st.markdown("""
    - Táº£i file **DOC hoáº·c DOCX** (cÃ³ thá»ƒ nhiá»u file cÃ¹ng lÃºc).
    - Nháº­p **tá»« khÃ³a** â†’ nháº¥n **Enter** hoáº·c **ğŸ” TÃ¬m kiáº¿m**.
    - á»¨ng dá»¥ng hiá»ƒn thá»‹ **Ä‘oáº¡n vÄƒn chá»©a tá»« khÃ³a**, má»Ÿ rá»™ng vÃ i cÃ¢u trÆ°á»›c/sau Ä‘á»ƒ Ä‘á»§ Ã½.
    - Giá»¯ nguyÃªn **ngáº¯t dÃ²ng, Ä‘á»‹nh dáº¡ng gá»‘c**.
    - Cá»¥m tá»« khÃ³a Ä‘Æ°á»£c **bÃ´i Ä‘á», in Ä‘áº­m** Ä‘á»ƒ dá»… nháº­n biáº¿t.
    """)
