import streamlit as st
from docx import Document
import mammoth
from unidecode import unidecode
import io
import re
import hashlib

# ==========================
# ‚öôÔ∏è C·∫§U H√åNH GIAO DI·ªÜN
# ==========================
st.set_page_config(
    page_title="üìÑ Tra c·ª©u vƒÉn b·∫£n Word",
    page_icon="üìò",
    layout="wide"
)

st.title("üìÑ ·ª®NG D·ª§NG TRA C·ª®U N·ªòI DUNG VƒÇN B·∫¢N (.DOC, .DOCX)")
st.markdown(
    """
    - üìÇ **B√™n tr√°i:** T·∫£i file `.doc`, `.docx` c·∫ßn tra c·ª©u.  
    - üîé **B√™n ph·∫£i:** Nh·∫≠p t·ª´ kh√≥a ‚Üí Nh·∫•n **Enter** ho·∫∑c n√∫t **"T√¨m ki·∫øm"** ƒë·ªÉ xem c√°c ƒëo·∫°n ch·ª©a t·ª´ kh√≥a k√®m ng·ªØ c·∫£nh 3‚Äì4 c√¢u.
    """
)

# ==========================
# ‚öôÔ∏è THAM S·ªê
# ==========================
CONTEXT_BEFORE = 3   # s·ªë c√¢u tr∆∞·ªõc t·ª´ kh√≥a
CONTEXT_AFTER = 3    # s·ªë c√¢u sau t·ª´ kh√≥a
MAX_RESULTS_PER_FILE = 200

# ==========================
# üß© C√ÅC H√ÄM X·ª¨ L√ù
# ==========================

def extract_text_from_docx(file_bytes: bytes) -> str:
    """ƒê·ªçc n·ªôi dung t·ª´ file .docx, gi·ªØ xu·ªëng d√≤ng gi·ªØa c√°c ƒëo·∫°n."""
    doc = Document(io.BytesIO(file_bytes))
    paragraphs = []
    for p in doc.paragraphs:
        text = p.text.strip()
        if text:
            paragraphs.append(text)
    return "\n".join(paragraphs)


def extract_text_from_doc(file_bytes: bytes) -> str:
    """ƒê·ªçc n·ªôi dung t·ª´ file .doc b·∫±ng mammoth, chuy·ªÉn HTML sang text."""
    result = mammoth.convert_to_html(io.BytesIO(file_bytes))
    html = result.value or ""
    # Lo·∫°i b·ªè tag HTML ƒë∆°n gi·∫£n
    text = re.sub(r"<[^>]+>", " ", html)
    # Chu·∫©n ho√° kho·∫£ng tr·∫Øng & xu·ªëng d√≤ng
    text = re.sub(r"\s+\n", "\n", text)
    text = re.sub(r"\n\s+", "\n", text)
    text = re.sub(r"\s{2,}", " ", text)
    return text.strip()


def split_into_sentences(text: str):
    """
    T√°ch c√¢u ƒë∆°n gi·∫£n, ∆∞u ti√™n nhanh & ·ªïn ƒë·ªãnh.
    V·∫´n gi·ªØ xu·ªëng d√≤ng b·∫±ng placeholder r·ªìi tr·∫£ l·∫°i.
    """
    if not text:
        return []

    # Chu·∫©n ho√° xu·ªëng d√≤ng
    normalized = text.replace("\r", "\n")
    normalized = re.sub(r"\n+", "\n", normalized)

    placeholder = "<NL>"
    normalized = normalized.replace("\n", f" {placeholder} ")

    # T√°ch sau c√°c d·∫•u . ! ? ‚Ä¶ ;
    parts = re.split(r'(?<=[\.!\?‚Ä¶;])\s+', normalized)

    sentences = []
    for part in parts:
        s = part.strip()
        if not s:
            continue
        s = s.replace(placeholder, "\n").strip()
        if len(s) > 0:
            sentences.append(s)

    # N·∫øu v√¨ l√Ω do n√†o ƒë√≥ kh√¥ng t√°ch ƒë∆∞·ª£c, coi to√†n b·ªô l√† 1 c√¢u
    if not sentences and text.strip():
        sentences = [text.strip()]

    return sentences


def normalize_for_search(text: str) -> str:
    """Chu·∫©n h√≥a ƒë·ªÉ so kh·ªõp: b·ªè d·∫•u, lower, remove d∆∞ kho·∫£ng tr·∫Øng."""
    return re.sub(r"\s+", " ", unidecode(text).lower()).strip()


def highlight_keyword(text: str, raw_keywords):
    """
    B√¥i v√†ng + in ƒë·∫≠m c√°c t·ª´ kh√≥a trong ƒëo·∫°n k·∫øt qu·∫£.
    D√πng t·ª´ kh√≥a g·ªëc (gi·ªØ d·∫•u), kh√¥ng ·∫£nh h∆∞·ªüng t·ªëc ƒë·ªô.
    """
    if not raw_keywords:
        return text

    # L·ªçc & s·∫Øp x·∫øp t·ª´ kh√≥a d√†i tr∆∞·ªõc
    keywords = sorted(
        {kw.strip() for kw in raw_keywords if kw.strip()},
        key=len,
        reverse=True
    )

    result = text
    for kw in keywords:
        pattern = re.escape(kw)
        regex = re.compile(pattern, flags=re.IGNORECASE)
        result = regex.sub(lambda m: f"<mark><b>{m.group(0)}</b></mark>", result)

    return result


# ==========================
# üß† X√ÇY D·ª∞NG CH·ªà M·ª§C (CACHE)
# ==========================

@st.cache_data(show_spinner=False)
def build_index(files_meta):
    """
    files_meta: danh s√°ch tuple (file_name, file_hash, file_bytes)

    Tr·∫£ v·ªÅ:
    [
      {
        "file_name": str,              # t√™n hi·ªÉn th·ªã
        "sentences": [str, ...],
        "norm_sentences": [str, ...],  # ƒë·ªÉ t√¨m nhanh
      },
      ...
    ]
    """
    indexed_docs = []

    for file_name, file_hash, file_bytes in files_meta:
        ext = file_name.lower().split(".")[-1]

        try:
            if ext == "docx":
                text = extract_text_from_docx(file_bytes)
            elif ext == "doc":
                text = extract_text_from_doc(file_bytes)
            else:
                continue

            if not text:
                continue

            sentences = split_into_sentences(text)
            if not sentences:
                continue

            norm_sentences = [normalize_for_search(s) for s in sentences]

            indexed_docs.append(
                {
                    "file_name": file_name,
                    "sentences": sentences,
                    "norm_sentences": norm_sentences,
                }
            )

        except Exception as e:
            st.warning(f"Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c file: {file_name}. L·ªói: {e}")

    return indexed_docs


def search_keyword(indexed_docs, query_raw: str,
                   before=3, after=3, max_results_per_file=200):
    """
    T√¨m theo t·ª´ kh√≥a, OR gi·ªØa c√°c t·ª´ kh√≥a.
    query_raw: chu·ªói, c√≥ th·ªÉ nhi·ªÅu t·ª´ kh√≥a, ph√¢n t√°ch ; ho·∫∑c ,
    """
    if not query_raw:
        return []

    # T√°ch danh s√°ch t·ª´ kh√≥a
    raw_parts = [p.strip() for p in re.split(r"[;,]", query_raw) if p.strip()]
    if not raw_parts:
        return []

    norm_keywords = [normalize_for_search(p) for p in raw_parts]

    results = []

    for doc in indexed_docs:
        file_name = doc["file_name"]
        sentences = doc["sentences"]
        norm_sentences = doc["norm_sentences"]

        hit_indices = []
        for i, s_norm in enumerate(norm_sentences):
            if any(kw and kw in s_norm for kw in norm_keywords):
                hit_indices.append(i)

        if not hit_indices:
            continue

        # Gom v√πng ng·ªØ c·∫£nh, tr√°nh tr√πng l·∫∑p
        merged_ranges = []
        for idx in hit_indices:
            start = max(0, idx - before)
            end = min(len(sentences), idx + after + 1)

            if merged_ranges and start <= merged_ranges[-1][1]:
                # G·ªôp v·ªõi v√πng tr∆∞·ªõc
                merged_ranges[-1] = (
                    merged_ranges[-1][0],
                    max(merged_ranges[-1][1], end),
                )
            else:
                merged_ranges.append((start, end))

        file_count = 0
        for start, end in merged_ranges:
            snippet_sentences = sentences[start:end]
            if not snippet_sentences:
                continue

            snippet_text = " ".join(snippet_sentences)
            snippet_text = re.sub(r"\s{2,}", " ", snippet_text).strip()

            # Highlight t·ª´ kh√≥a
            snippet_html = highlight_keyword(snippet_text, raw_parts)

            results.append(
                {
                    "file_name": file_name,
                    "context_html": snippet_html
                }
            )

            file_count += 1
            if file_count >= max_results_per_file:
                break

    return results


# ==========================
# üñ•Ô∏è GIAO DI·ªÜN 2 C·ªòT
# ==========================

col1, col2 = st.columns([1, 2])

# --- C·ªòT TR√ÅI: UPLOAD ---
with col1:
    st.subheader("üìÇ T·∫£i vƒÉn b·∫£n")
    uploaded_files = st.file_uploader(
        "Ch·ªçn m·ªôt ho·∫∑c nhi·ªÅu file .doc / .docx",
        type=["doc", "docx"],
        accept_multiple_files=True
    )

    if uploaded_files:
        st.success(f"ƒê√£ t·∫£i {len(uploaded_files)} file:")
        for f in uploaded_files:
            st.markdown(f"- `{f.name}` ({f.size/1024:.1f} KB)")
    else:
        st.info("Vui l√≤ng t·∫£i l√™n √≠t nh·∫•t m·ªôt file ƒë·ªÉ b·∫Øt ƒë·∫ßu tra c·ª©u.")

# --- C·ªòT PH·∫¢I: T√åM KI·∫æM ---
with col2:
    st.subheader("üîç Tra c·ª©u t·ª´ kh√≥a")

    with st.form("search_form", clear_on_submit=False):
        default_query = st.session_state.get("last_query", "")
        query = st.text_input(
            "Nh·∫≠p t·ª´ kh√≥a (c√≥ th·ªÉ nhi·ªÅu, c√°ch nhau b·ªüi ';' ho·∫∑c ',')",
            value=default_query,
            placeholder="V√≠ d·ª•: h·∫°n m·ª©c t√≠n d·ª•ng; t√†i s·∫£n b·∫£o ƒë·∫£m; ƒëi·ªÅu ki·ªán vay"
        )
        submitted = st.form_submit_button("üîç T√¨m ki·∫øm")

    if submitted:
        st.session_state["last_query"] = query

        if not uploaded_files:
            st.warning("Vui l√≤ng t·∫£i file ·ªü b√™n tr√°i tr∆∞·ªõc khi t√¨m ki·∫øm.")
        elif not query.strip():
            st.warning("Vui l√≤ng nh·∫≠p t·ª´ kh√≥a c·∫ßn tra c·ª©u.")
        else:
            # Chu·∫©n b·ªã d·ªØ li·ªáu cho cache: d√πng hash ƒë·ªÉ nh·∫≠n di·ªán phi√™n b·∫£n file
            files_meta = []
            for uf in uploaded_files:
                content = uf.getvalue()
                file_hash = hashlib.md5(content).hexdigest()
                # KH√îNG ch·ªânh s·ª≠a t√™n file khi ƒë·ªçc ƒëu√¥i, ch·ªâ d√πng hash cho cache
                files_meta.append((uf.name, file_hash, content))

            with st.spinner("ƒêang x·ª≠ l√Ω & tra c·ª©u..."):
                indexed_docs = build_index(tuple(files_meta))
                results = search_keyword(
                    indexed_docs,
                    query_raw=query,
                    before=CONTEXT_BEFORE,
                    after=CONTEXT_AFTER,
                    max_results_per_file=MAX_RESULTS_PER_FILE
                )

            st.markdown("---")

            if not results:
                st.warning("Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£ n√†o ch·ª©a t·ª´ kh√≥a trong c√°c file ƒë√£ t·∫£i.")
            else:
                st.success(f"T√¨m th·∫•y {len(results)} ƒëo·∫°n ph√π h·ª£p trong c√°c vƒÉn b·∫£n.")
                for i, item in enumerate(results, start=1):
                    st.markdown(
                        f"""
                        <div style="padding:10px; margin-bottom:10px; border-radius:6px; border:1px solid #ddd;">
                            <div style="font-size:12px; color:#666;">
                                <b>File:</b> {item['file_name']} | <b>K·∫øt qu·∫£ #{i}</b>
                            </div>
                            <div style="margin-top:6px; font-size:14px; line-height:1.6; text-align:justify;">
                                {item['context_html']}
                            </div>
                        </div>
                        """,
                        unsafe_allow_html=True
                    )
